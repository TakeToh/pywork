{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下テンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 表示用\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Parametor about Sensor\n",
    "SensorNum=1 # Sensor number\n",
    "#SensorAxis='AccX' # Axis\n",
    "\n",
    "# Define Data Name\n",
    "DATE= str( datetime.date.today() )+'/'\n",
    "TITLE=\"Participants01_First_1/\"\n",
    "Label = ['LeftHand','RightHand','LeftLeg','Rightleg','West','Chest']\n",
    "\n",
    "# Parametor about \" Data Path\"\n",
    "DataPath = \"/media/takeyama/Transfer/02_ActivityResearch/Paticipants01/01_First/20161018-105301/mem/\"\n",
    "StorePath = \"/media/takeyama/Transfer/02_ActivityResearch/\"\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+'dictionary/'): \n",
    "    os.makedirs(StorePath+TITLE+'dictionary/')\n",
    "DictionaryDataPath=StorePath+TITLE+'dictionary/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+'window/'): \n",
    "    os.makedirs(StorePath+TITLE+'window/')\n",
    "WindowDataPath=StorePath+TITLE+'window/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'studyOutput/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'studyOutput/')\n",
    "StudyOutputPath=StorePath+TITLE+DATE+'studyOutput/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'parametor/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'parametor/')\n",
    "ParametorPath=StorePath+TITLE+DATE+'parametor/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'result/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'result/')\n",
    "ResultPath=StorePath+TITLE+DATE+'result/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'graph/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'graph/')\n",
    "GraphPath=StorePath+TITLE+DATE+'graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**センサデータの読み込み**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# センサデータのcsvファイル名をリストに集約する関数\n",
    "def SensorDataFileNameList(path):\n",
    "    l = os.listdir(DataPath)\n",
    "    FilePath =[]\n",
    "    SensorName = []\n",
    "    \n",
    "    for n in range(len(l)):\n",
    "        if (l[n][:4].find('mem-') != -1):\n",
    "            FilePath.append(l[n])\n",
    "            SensorName.append(l[n][4:15])\n",
    "    \n",
    "    return FilePath,SensorName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSVを読み込んで，CSVの中身を辞書型で表現する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ImportCSV(csv_file,SensorName,mode='Round'):\n",
    "    import pandas as pd\n",
    "    '''\n",
    "    ~Argument~\n",
    "    csv_file -> ファイル名 \n",
    "    mode Round -> 四捨五入\n",
    "         Roundup -> 切り上げ\n",
    "         Rounddown -> 切り捨て\n",
    "    \n",
    "    ~Conversion~\n",
    "    Acc Data  [0.1mG]=>[G]\n",
    "    Gyr Data  [0.01dps]=>[dps]   ...dps=degree per second\n",
    "    '''\n",
    "    # data dictionary \n",
    "    RawData={}   \n",
    "    AccConversion = 0.1 * 0.001\n",
    "    GyrConversion = 0.01\n",
    "    \n",
    "    # design dataframe and import csv\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data.columns=[u'Type',u'Time',u'AccX',u'AccY',u'AccZ',u'GyrX',u'GyrY',u'GyrZ']\n",
    "    data = data[ data['Type']=='ags']\n",
    "\n",
    "    # convert numpy.darray \n",
    "\n",
    "    AccX=data.AccX.values*AccConversion\n",
    "    AccY=data.AccY.values*AccConversion\n",
    "    AccZ=data.AccZ.values*AccConversion\n",
    "    \n",
    "    GyrX=data.GyrX.values*GyrConversion\n",
    "    GyrY=data.GyrY.values*GyrConversion\n",
    "    GyrZ=data.GyrZ.values*GyrConversion\n",
    "\n",
    "    # regist each raw data \n",
    "    RawData['AccX'] = AccX\n",
    "    RawData['AccY'] = AccY\n",
    "    RawData['AccZ'] = AccZ\n",
    "    RawData['GyrX'] = GyrX\n",
    "    RawData['GyrY'] = GyrY\n",
    "    RawData['GyrZ'] = GyrZ\n",
    "    RawData['Name'] = SensorName\n",
    "\n",
    "    RawData['Time'] = data.Time.values\n",
    "\n",
    "    return RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FFT library\n",
    "from scipy import fftpack\n",
    "from scipy import signal\n",
    "\n",
    "# Graph Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate WindowFrame Function\n",
    "from mymodule import window\n",
    "\n",
    "# timer\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential#,Graph\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/takeyama/Transfer/02_ActivityResearch/Participants01_First_1/dictionary/\n"
     ]
    }
   ],
   "source": [
    "print DictionaryDataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chest.npz', 'LeftHand.npz', 'LeftLeg.npz', 'RightHand.npz', 'Rightleg.npz', 'West.npz']\n"
     ]
    }
   ],
   "source": [
    "DictName=os.listdir(DictionaryDataPath)\n",
    "print DictName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sensor1 = np.load(DictionaryDataPath+DictName[0])['data'][()]\n",
    "Sensor2 = np.load(DictionaryDataPath+DictName[1])['data'][()]\n",
    "Sensor3 = np.load(DictionaryDataPath+DictName[2])['data'][()]\n",
    "Sensor4 = np.load(DictionaryDataPath+DictName[3])['data'][()]\n",
    "Sensor5 = np.load(DictionaryDataPath+DictName[4])['data'][()]\n",
    "Sensor6 = np.load(DictionaryDataPath+DictName[5])['data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetWindowFrame(windowName, dataRaw, wWidth, sWidth, PATH):\n",
    "    \"\"\"\n",
    "    data　ウィンドウフレームに変換するデータ\n",
    "    registName　ウィンドウフレームに登録するデータの名前\n",
    "    windowWidth　ウィンドウ幅\n",
    "    slidingWidth　スライド幅\n",
    "    PATH ウィドウフレームを保存するディレクトリ\n",
    "    \n",
    "    \n",
    "    About Function:\n",
    "        与えられたdataからウィンドウ幅windowWidth,スライド幅slidingWidthにしたがって\n",
    "        registNameのウィドウフレームを返す．\n",
    "        また，与えれたPATH内に同様なパラメータ( WindowWidth, slidingWidth)かつ同様な\n",
    "        windowNameのものがある場合，そのデータを返す．\n",
    "        この関数が登録，ウィンドウフレームに変換できるデータは１つとする\n",
    "    \"\"\"\n",
    "    storedName = windowName+'_Win='+str(wWidth).zfill(4)+'_Sld='+str(sWidth).zfill(4)+'.npz'\n",
    "    l = os.listdir(PATH)\n",
    "    \n",
    "    if storedName in l:\n",
    "        print \"this data had finished making\"\n",
    "        return np.load(PATH+storedName)['data'][()]\n",
    "    \n",
    "    w=window()\n",
    "    w.SetData(windowName,dataRaw)    \n",
    "    wind=w.Compile(wWidth,sWidth)\n",
    "    windoW=wind.reshape((len(wind),np.prod(wind.shape[1:])))\n",
    "    \n",
    "    np.savez(PATH+storedName,data=wind)\n",
    "\n",
    "    return wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine\n",
    "https://github.com/benanne/morb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named morb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b2e1bebd92e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmorb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named morb"
     ]
    }
   ],
   "source": [
    "\"\"\"This tutorial introduces restricted boltzmann machines (RBM) using Theano.\n",
    "\n",
    "Boltzmann Machines (BMs) are a particular form of energy-based model which\n",
    "contain hidden variables. Restricted Boltzmann Machines further restrict BMs\n",
    "to those without visible-visible and hidden-hidden connections.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import timeit\n",
    "\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from utils import tile_raster_images\n",
    "from logistic_sgd import load_data\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if numpy_rng is None: numpy_rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        if theano_rng is None: theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        self.theano_rng = theano_rng\n",
    "        \n",
    "        if W is None:\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)),\n",
    "                dtype=theano.config.floatX)\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "        self.W = W\n",
    "\n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),name='hbias', borrow=True)\n",
    "        self.hbias = hbias\n",
    "\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX),name='vbias', borrow=True)\n",
    "        self.vbias = vbias\n",
    "\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "        self.input = input\n",
    "\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        # end-snippet-1\n",
    "\n",
    "    def free_energy(self, v_sample):\n",
    "        ''' Function to compute the free energy '''\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    def propup(self, vis):\n",
    "        '''This function propagates the visible units activation upwards to\n",
    "        the hidden units\n",
    "\n",
    "        Note that we return also the pre-sigmoid activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        ''' This function infers state of hidden units given visible units '''\n",
    "        # compute the activation of the hidden units given a sample of\n",
    "        # the visibles\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        # get a sample of the hiddens given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        '''This function propagates the hidden units activation downwards to\n",
    "        the visible units\n",
    "\n",
    "        Note that we return also the pre_sigmoid_activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        ''' This function infers state of visible units given hidden units '''\n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the hidden state'''\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the visible state'''\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # start-snippet-2\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
    "\n",
    "        :param lr: learning rate used to train the RBM\n",
    "\n",
    "        :param persistent: None for CD. For PCD, shared variable\n",
    "            containing old state of Gibbs chain. This must be a shared\n",
    "            variable of size (batch size, number of hidden units).\n",
    "\n",
    "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
    "\n",
    "        Returns a proxy for the cost and the updates dictionary. The\n",
    "        dictionary contains the update rules for weights and biases but\n",
    "        also an update of the shared variable used to store the persistent\n",
    "        chain, if one is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "        # end-snippet-2\n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        # Read Theano tutorial on scan for more information :\n",
    "        # http://deeplearning.net/software/theano/library/scan.html\n",
    "        # the scan will return the entire Gibbs chain\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k,\n",
    "            name=\"gibbs_hvh\"\n",
    "        )\n",
    "        # start-snippet-3\n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "            self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "        # end-snippet-3 start-snippet-4\n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(\n",
    "                lr,\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                           pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "        # end-snippet-4\n",
    "\n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
    "\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n",
    "                                                            fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n",
    "        \"\"\"Approximation to the reconstruction error\n",
    "\n",
    "        Note that this function requires the pre-sigmoid activation as\n",
    "        input.  To understand why this is so you need to understand a\n",
    "        bit about how Theano works. Whenever you compile a Theano\n",
    "        function, the computational graph that you pass as input gets\n",
    "        optimized for speed and stability.  This is done by changing\n",
    "        several parts of the subgraphs with others.  One such\n",
    "        optimization expresses terms of the form log(sigmoid(x)) in\n",
    "        terms of softplus.  We need this optimization for the\n",
    "        cross-entropy since sigmoid of numbers larger than 30. (or\n",
    "        even less then that) turn to 1. and numbers smaller than\n",
    "        -30. turn to 0 which in terms will force theano to compute\n",
    "        log(0) and therefore we will get either -inf or NaN as\n",
    "        cost. If the value is expressed in terms of softplus we do not\n",
    "        get this undesirable behaviour. This optimization usually\n",
    "        works fine, but here we have a special case. The sigmoid is\n",
    "        applied inside the scan op, while the log is\n",
    "        outside. Therefore Theano will only see log(scan(..)) instead\n",
    "        of log(sigmoid(..)) and will not apply the wanted\n",
    "        optimization. We can not go and replace the sigmoid in scan\n",
    "        with something else also, because this only needs to be done\n",
    "        on the last step. Therefore the easiest and more efficient way\n",
    "        is to get also the pre-sigmoid activation as an output of\n",
    "        scan, and apply both the log and sigmoid outside scan such\n",
    "        that Theano can catch and optimize the expression.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "def test_rbm(learning_rate=0.1, training_epochs=15,\n",
    "             dataset='mnist.pkl.gz', batch_size=20,\n",
    "             n_chains=20, n_samples=10, output_folder='rbm_plots',\n",
    "             n_hidden=500):\n",
    "    \"\"\"\n",
    "    Demonstrate how to train and afterwards sample from it using Theano.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :param learning_rate: learning rate used for training the RBM\n",
    "\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :param dataset: path the the pickled dataset\n",
    "\n",
    "    :param batch_size: size of a batch used to train the RBM\n",
    "\n",
    "    :param n_chains: number of parallel Gibbs chains to be used for sampling\n",
    "\n",
    "    :param n_samples: number of samples to plot for each chain\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # initialize storage for the persistent chain (state = hidden\n",
    "    # layer of chain)\n",
    "    persistent_chain = theano.shared(numpy.zeros((batch_size, n_hidden),\n",
    "                                                 dtype=theano.config.floatX),\n",
    "                                     borrow=True)\n",
    "\n",
    "    # construct the RBM class\n",
    "    rbm = RBM(input=x, n_visible=28 * 28,\n",
    "              n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
    "\n",
    "    # get the cost and the gradient corresponding to one step of CD-15\n",
    "    cost, updates = rbm.get_cost_updates(lr=learning_rate,\n",
    "                                         persistent=persistent_chain, k=15)\n",
    "\n",
    "    #################################\n",
    "    #     Training the RBM          #\n",
    "    #################################\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "\n",
    "    # start-snippet-5\n",
    "    # it is ok for a theano function to have no output\n",
    "    # the purpose of train_rbm is solely to update the RBM parameters\n",
    "    train_rbm = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            tile_raster_images(\n",
    "                X=rbm.W.get_value(borrow=True).T,\n",
    "                img_shape=(28, 28),\n",
    "                tile_shape=(10, 10),\n",
    "                tile_spacing=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "    # end-snippet-5 start-snippet-6\n",
    "    #################################\n",
    "    #     Sampling from the RBM     #\n",
    "    #################################\n",
    "    # find out the number of test samples\n",
    "    number_of_test_samples = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(number_of_test_samples - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x.get_value(borrow=True)[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    # end-snippet-6 start-snippet-7\n",
    "    plot_every = 1000\n",
    "    # define one step of Gibbs sampling (mf = mean-field) define a\n",
    "    # function that does `plot_every` steps before returning the\n",
    "    # sample for plotting\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every,\n",
    "        name=\"gibbs_vhv\"\n",
    "    )\n",
    "\n",
    "    # add to updates the shared variable that takes care of our persistent\n",
    "    # chain :.\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]})\n",
    "    # construct the function that implements our persistent chain.\n",
    "    # we generate the \"mean field\" activations for plotting and the actual\n",
    "    # samples for reinitializing the state of our persistent chain\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting ( we need to leave\n",
    "    # room for the tile_spacing as well)\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in range(n_samples):\n",
    "        # generate `plot_every` intermediate samples that we discard,\n",
    "        # because successive samples in the chain are too correlated\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print(' ... plotting sample %d' % idx)\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    # construct image\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')\n",
    "    # end-snippet-7\n",
    "    os.chdir('../')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
