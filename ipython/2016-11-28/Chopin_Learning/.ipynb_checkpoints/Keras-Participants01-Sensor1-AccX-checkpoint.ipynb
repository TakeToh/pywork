{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下テンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 表示用\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Parametor about Sensor\n",
    "SensorNum=1 # Sensor number\n",
    "#SensorAxis='AccX' # Axis\n",
    "\n",
    "# Define Data Name\n",
    "DATE= str( datetime.date.today() )+'/'\n",
    "TITLE=\"Participants01_First_1/\"\n",
    "Label = ['LeftHand','RightHand','LeftLeg','Rightleg','West','Chest']\n",
    "\n",
    "# Parametor about \" Data Path\"\n",
    "DataPath = \"/media/takeyama/Transfer/02_ActivityResearch/Paticipants01/01_First/20161018-105301/mem/\"\n",
    "StorePath = \"/home/takeyama/Documents/\"\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+'dictionary/'): \n",
    "    os.makedirs(StorePath+TITLE+'dictionary/')\n",
    "DictionaryDataPath=StorePath+TITLE+'dictionary/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+'window/'): \n",
    "    os.makedirs(StorePath+TITLE+'window/')\n",
    "WindowDataPath=StorePath+TITLE+'window/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'studyOutput/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'studyOutput/')\n",
    "StudyOutputPath=StorePath+TITLE+DATE+'studyOutput/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'parametor/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'parametor/')\n",
    "ParametorPath=StorePath+TITLE+DATE+'parametor/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'result/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'result/')\n",
    "ResultPath=StorePath+TITLE+DATE+'result/'\n",
    "\n",
    "if not os.path.exists(StorePath+TITLE+DATE+'graph/'): \n",
    "    os.makedirs(StorePath+TITLE+DATE+'graph/')\n",
    "GraphPath=StorePath+TITLE+DATE+'graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**センサデータの読み込み**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# センサデータのcsvファイル名をリストに集約する関数\n",
    "def SensorDataFileNameList(path):\n",
    "    l = os.listdir(DataPath)\n",
    "    FilePath =[]\n",
    "    SensorName = []\n",
    "    \n",
    "    for n in range(len(l)):\n",
    "        if (l[n][:4].find('mem-') != -1):\n",
    "            FilePath.append(l[n])\n",
    "            SensorName.append(l[n][4:15])\n",
    "    \n",
    "    return FilePath,SensorName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSVを読み込んで，CSVの中身を辞書型で表現する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ImportCSV(csv_file,SensorName,mode='Round'):\n",
    "    import pandas as pd\n",
    "    '''\n",
    "    ~Argument~\n",
    "    csv_file -> ファイル名 \n",
    "    mode Round -> 四捨五入\n",
    "         Roundup -> 切り上げ\n",
    "         Rounddown -> 切り捨て\n",
    "    \n",
    "    ~Conversion~\n",
    "    Acc Data  [0.1mG]=>[G]\n",
    "    Gyr Data  [0.01dps]=>[dps]   ...dps=degree per second\n",
    "    '''\n",
    "    # data dictionary \n",
    "    RawData={}   \n",
    "    AccConversion = 0.1 * 0.001\n",
    "    GyrConversion = 0.01\n",
    "    \n",
    "    # design dataframe and import csv\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data.columns=[u'Type',u'Time',u'AccX',u'AccY',u'AccZ',u'GyrX',u'GyrY',u'GyrZ']\n",
    "    data = data[ data['Type']=='ags']\n",
    "\n",
    "    # convert numpy.darray \n",
    "\n",
    "    AccX=data.AccX.values*AccConversion\n",
    "    AccY=data.AccY.values*AccConversion\n",
    "    AccZ=data.AccZ.values*AccConversion\n",
    "    \n",
    "    GyrX=data.GyrX.values*GyrConversion\n",
    "    GyrY=data.GyrY.values*GyrConversion\n",
    "    GyrZ=data.GyrZ.values*GyrConversion\n",
    "\n",
    "    # regist each raw data \n",
    "    RawData['AccX'] = AccX\n",
    "    RawData['AccY'] = AccY\n",
    "    RawData['AccZ'] = AccZ\n",
    "    RawData['GyrX'] = GyrX\n",
    "    RawData['GyrY'] = GyrY\n",
    "    RawData['GyrZ'] = GyrZ\n",
    "    RawData['Name'] = SensorName\n",
    "\n",
    "    RawData['Time'] = data.Time.values\n",
    "\n",
    "    return RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FFT library\n",
    "from scipy import fftpack\n",
    "from scipy import signal\n",
    "\n",
    "# Graph Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate WindowFrame Function\n",
    "from mymodule import window\n",
    "\n",
    "# timer\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase1 計測データの取得**\n",
    "\n",
    "ここで行うことは，学習に用いるnpzファイルを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DictName=os.listdir(DictionaryDataPath)\n",
    "Sensor1 = np.load(DictionaryDataPath+DictName[0])['data'][()]\n",
    "Sensor2 = np.load(DictionaryDataPath+DictName[1])['data'][()]\n",
    "Sensor3 = np.load(DictionaryDataPath+DictName[2])['data'][()]\n",
    "Sensor4 = np.load(DictionaryDataPath+DictName[3])['data'][()]\n",
    "Sensor5 = np.load(DictionaryDataPath+DictName[4])['data'][()]\n",
    "Sensor6 = np.load(DictionaryDataPath+DictName[5])['data'][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase2 window flame 作成 & Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AutoEncoder(TrainWindow,TestWindow,dim,opt,cname,batchSize):\n",
    "    \"\"\"\n",
    "    TrainWindow: 訓練データ\n",
    "    TestWindow: テストデータ\n",
    "    dim: 中間層の次元数\n",
    "    opt: 最適化関数\n",
    "    cname: 各種データを保存する用の共通の名前\n",
    "    \"\"\"\n",
    "    input_img = Input(shape=( TrainWindow.shape[1] ,))\n",
    "    encoded = Dense(dim, activation='tanh')(input_img)\n",
    "    decoded = Dense( TrainWindow.shape[1] , activation='linear')(encoded)\n",
    "    autoencoder = Model(input=input_img, output=decoded)\n",
    "    encoder = Model(input=input_img, output=encoded)\n",
    "    encoded_input = Input(shape=(dim,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    #plot(autoencoder,  to_file=StudyDataModelPicPath+SaveFileNameNet+'.png')\n",
    "    \n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    hist = autoencoder.fit(TrainWindow, TestWindow,\n",
    "                    nb_epoch=50,\n",
    "                    verbose=2,\n",
    "                    batch_size=batchSize,\n",
    "                    shuffle=False,\n",
    "                    validation_data=(TrainWindow, TestWindow),\n",
    "                    #callbacks=[early_stopping]\n",
    "                          )\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    encoded_imgs = encoder.predict(TrainWindow,batch_size=batchSize,verbose=1)\n",
    "    decoded_imgs = decoder.predict(encoded_imgs,batch_size=batchSize,verbose=1)\n",
    "\n",
    "    np.savez(StudyOutputPath+cname+'_Encoded',data=encoded_imgs)\n",
    "    np.savez(StudyOutputPath+cname+'_Decoded',data=decoded_imgs)\n",
    "\n",
    "    # save model and wights\n",
    "    json_string = encoder.to_json()\n",
    "    open(StudyOutputPath+cname+'_Encoded'+'.json', 'w').write(json_string)\n",
    "    encoder.save_weights(ParametorPath+cname+'Encode_weights.h5',overwrite=True)\n",
    "\n",
    "    json_string = decoder.to_json()\n",
    "    open(StudyOutputPath+cname+'_Decoded'+'.json', 'w').write(json_string)\n",
    "    decoder.save_weights(ParametorPath+cname+'Decord_weights.h5',overwrite=True)\n",
    "\n",
    "    json_string = autoencoder.to_json()\n",
    "    open(StudyOutputPath+cname+'_Net'+'.json', 'w').write(json_string)\n",
    "    autoencoder.save_weights(ParametorPath+cname+'Net_weights.h5',overwrite=True)\n",
    "\n",
    "    # plot loss\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "\n",
    "    nb_epoch = len(loss)\n",
    "    plt.plot(range(nb_epoch), loss, marker='.', label='loss')\n",
    "    plt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(ResultPath+cname+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetWindowFrame(windowName, dataRaw, wWidth, sWidth, PATH):\n",
    "    \"\"\"\n",
    "    data　ウィンドウフレームに変換するデータ\n",
    "    registName　ウィンドウフレームに登録するデータの名前\n",
    "    windowWidth　ウィンドウ幅\n",
    "    slidingWidth　スライド幅\n",
    "    PATH ウィドウフレームを保存するディレクトリ\n",
    "    \n",
    "    \n",
    "    About Function:\n",
    "        与えられたdataからウィンドウ幅windowWidth,スライド幅slidingWidthにしたがって\n",
    "        registNameのウィドウフレームを返す．\n",
    "        また，与えれたPATH内に同様なパラメータ( WindowWidth, slidingWidth)かつ同様な\n",
    "        windowNameのものがある場合，そのデータを返す．\n",
    "        この関数が登録，ウィンドウフレームに変換できるデータは１つとする\n",
    "    \"\"\"\n",
    "    storedName = windowName+'_Win='+str(wWidth).zfill(4)+'_Sld='+str(sWidth).zfill(4)+'.npz'\n",
    "    l = os.listdir(PATH)\n",
    "    \n",
    "    if storedName in l:\n",
    "        print \"this data had finished making\"\n",
    "        return np.load(PATH+storedName)['data'][()]\n",
    "    \n",
    "    w=window()\n",
    "    w.SetData(windowName,dataRaw)    \n",
    "    wind=w.Compile(wWidth,sWidth)\n",
    "    windoW=wind.reshape((len(wind),np.prod(wind.shape[1:])))\n",
    "    \n",
    "    np.savez(PATH+storedName,data=wind)\n",
    "\n",
    "    return wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this data had finished making\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.8825,  0.8755,  0.8711, ...,  0.9233,  0.925 ,  0.9206],\n",
       "       [ 0.8674,  0.8728,  0.8821, ...,  0.8264,  0.824 ,  0.8271],\n",
       "       [ 0.8371,  0.8437,  0.8447, ...,  0.8532,  0.8471,  0.8498],\n",
       "       ..., \n",
       "       [-0.1003, -0.0998, -0.0912, ..., -0.0995, -0.0942, -0.0925],\n",
       "       [-0.1   , -0.1025, -0.1022, ..., -0.0939, -0.0986, -0.0966],\n",
       "       [-0.0939, -0.0978, -0.0981, ..., -0.0976, -0.1015, -0.0973]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetWindowFrame('AccX',Sensor1['AccX'],256,256/8,WindowDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクリプトの目的\n",
    "本スクリプトの目的および学習方法を明記する\n",
    "本スクリプトの目的は，**学習パラメータを今までのと比較して大きくことから，学習にどのような影響を与えるのか確認する**\n",
    "\n",
    "パラメータ\n",
    "* window幅 256,512,1024,2048,4096\n",
    "* slidign幅 window幅/8\n",
    "* Batch_size 32\n",
    "* Optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-8500d4a3c1cf>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-8500d4a3c1cf>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    CommonName='_dim='+str(AEDimention)+'_Win='+str(WindowNum).zfill(4)+                '_Sld='+str(SlidingNum).zfill(4)+'_'+Optim                '_BatchSize='+str(BatchSize).zfill(3)\u001b[0m\n\u001b[0m                                                                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# loop windowWidth\n",
    "WindowWidth = np.array( [256,512,1024,2048,4096] )\n",
    "SlidingWidth = WindowWidth/8\n",
    "BatchSize = 32\n",
    "Optim='Adam'\n",
    "AEDimention = 16\n",
    "\n",
    "for WindowNum,SlidingNum in zip(WindowWidth,SlidingWidth):\n",
    "    windoW = GetWindowFrame('AccX',Sensor1['AccX'],WindowNum,SlidingNum,WindowDataPath)\n",
    "    \n",
    "    print \"hiden node = \"+str(AEDimention)\n",
    "    CommonName='_dim='+str(AEDimention)+'_Win='+str(WindowNum).zfill(4)+'_Sld='+str(SlidingNum).zfill(4)+\\\n",
    "    '_'+Optim+'_BatchSize='+str(BatchSize).zfill(3)\n",
    "    print CommonName\n",
    "\n",
    "    #np.savez(WindowDataPath+CommonName+'_Test',data=windoW)\n",
    "    #np.savez(WindowDataPath+CommonName+'_Train',data=windoW)\n",
    "\n",
    "    AutoEncoder(windoW,windoW,AEDimention,Optim,CommonName,BatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print 'finish'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "手洗いの識別にて「タイムスライス」という言葉が出た．意味としては，Native Baisian Networkにて，あるデータiについての確率を過去からのデータから導出する際にその過去がどれくらいの過去なのかを示す．つまりP($X_i$)をP($X_(i-1)$)からP($X_{i-T_n})$)から導出するとした時の$T_n$がタイムスライスである．\n",
    "\n",
    "これは学習で言うバッチサイズと似ているのではないかと考える．今回学習は訓練データをシャッフルしてない．つまり，学習する際の訓練データの時系列は連続であると言える．\n",
    "\n",
    "よって，件の識別はタイムスライスの値によって識別率は変化するらしい．ならば，類似していると仮定するバッチサイズの大きさによって識別結果も変わるかどうか検証する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop windowWidth\n",
    "l=len(WindowWidth)*len(optimArray)\n",
    "pbar = tqdm_notebook(total=l)\n",
    "i=0\n",
    "for WindowNum,SlidingNum in zip(WindowWidth,SlidingWidth):\n",
    "    w=window()\n",
    "    w.SetData('AccX',TrimData[0]['AccX'])\n",
    "    wind=w.Compile(windowWidth=WindowNum,slidingWidth=SlidingNum)\n",
    "    windoW=wind.reshape((len(wind),np.prod(wind.shape[1:])))\n",
    "    \n",
    "    for bs in [1,2,4,8,16,32]:\n",
    "        print \"hiden node = \"+str(AEDimention)\n",
    "        CommonName='_dim='+str(AEDimention).zfill(4)+'_Win='+str(WindowNum).zfill(4)\n",
    "        CommonName=CommonName+'_Sld='+str(SlidingNum).zfill(4)+'_'+optName\n",
    "        CommonName=CommonName+'BatchSize='+str(bs).zfill(2)\n",
    "        print CommonName\n",
    "        \n",
    "        np.savez(WindowDataPath+CommonName+'_Test',data=windoW)\n",
    "        np.savez(WindowDataPath+CommonName+'_Train',data=windoW)\n",
    "        \n",
    "        AutoEncoder(windoW,windoW,AEDimention,optimArray[2],CommonName,batchSize=bs)\n",
    "        \n",
    "        pbar.update(i+1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
